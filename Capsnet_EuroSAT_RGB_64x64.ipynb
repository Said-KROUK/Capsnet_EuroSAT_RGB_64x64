{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import des bibliothèques nécessaires\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import initializers, layers, models, optimizers, callbacks\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "TrpM9rPw9WKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Calcule la longueur des vecteurs.\n",
        "    Utilisé pour produire un tenseur de même forme que `y_true` dans `margin_loss`.\n",
        "\n",
        "    Entrée :\n",
        "        - inputs : Tenseur de forme [dim_1, ..., dim_{n-1}, dim_n].\n",
        "    Sortie :\n",
        "        - Tenseur de forme [dim_1, ..., dim_{n-1}].\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return K.sqrt(K.sum(K.square(inputs), -1))  # Longueur euclidienne des vecteurs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]  # La sortie a la même forme que l'entrée sauf la dernière dimension\n"
      ],
      "metadata": {
        "id": "n1YZGCnv9uIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask un tenseur avec la forme = [None, num_capsule, dim_vector] en masquant soit la capsule avec la longueur maximale,\n",
        "    soit en utilisant un masque d'entrée additionnel. Toutes les autres capsules sont mises à zéro.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Mask, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if isinstance(inputs, list):  # true label est fourni avec la forme = [None, n_classes], c'est-à-dire un code one-hot.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # si aucun label n'est fourni, on masque par la capsule de longueur maximale. Utilisé principalement pour la prédiction\n",
        "            # calcul des longueurs des capsules\n",
        "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
        "            # génère le masque qui est un code one-hot.\n",
        "            # mask.shape = [None, n_classes] = [None, num_capsule]\n",
        "            mask = tf.one_hot(indices=tf.argmax(x, 1), depth=tf.shape(inputs)[1])\n",
        "\n",
        "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_capsule]\n",
        "        masked = layers.Flatten()(inputs * K.expand_dims(mask, -1))\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape[0], tuple):  # true label fourni\n",
        "            return (None, input_shape[0][1] * input_shape[0][2])\n",
        "        else:  # pas de true label fourni\n",
        "            return (None, input_shape[1] * input_shape[2])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Mask, self).get_config()\n",
        "        return config\n",
        "\n"
      ],
      "metadata": {
        "id": "e5eHirli9wxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    Fonction d'activation non linéaire utilisée dans les capsules.\n",
        "    Elle rapproche la longueur d'un grand vecteur de 1 et celle d'un petit vecteur de 0.\n",
        "\n",
        "    :param vectors: Vecteurs à transformer.\n",
        "    :param axis: Axe selon lequel appliquer la transformation.\n",
        "    :return: Tenseur de même forme que l'entrée.\n",
        "    \"\"\"\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)  # Norme au carré\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())  # Facteur d'échelle\n",
        "    return scale * vectors"
      ],
      "metadata": {
        "id": "36QUHdwa92AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frJgguLN7-lt"
      },
      "outputs": [],
      "source": [
        "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
        "    \"\"\"\n",
        "    Applique Conv2D `n_channels` fois et concatène toutes les capsules.\n",
        "\n",
        "    :param inputs: Tenseur 4D, de forme [None, largeur, hauteur, canaux].\n",
        "    :param dim_vector: Dimension des vecteurs de sortie.\n",
        "    :param n_channels: Nombre de types de capsules.\n",
        "    :return: Tenseur de sortie, forme [None, num_capsule, dim_vector].\n",
        "    \"\"\"\n",
        "    output = layers.Conv2D(filters=dim_vector * n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
        "                           name='primarycap_conv2d')(inputs)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def margin_loss(labels, raw_logits, margin=0.4, downweight=0.5):\n",
        "    \"\"\"\n",
        "    Fonction de perte Margin pour Capsule Network.\n",
        "\n",
        "    Args:\n",
        "        labels: Tensor, les labels réels en encoding one-hot (shape: [batch_size, num_classes]).\n",
        "        raw_logits: Tensor, prédictions du modèle dans l'intervalle [0, 1].\n",
        "        margin: Scalar, la marge après soustraction de 0.5 des logits (par défaut 0.4).\n",
        "        downweight: Scalar, facteur d'atténuation pour le coût des valeurs négatives.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: La perte totale calculée pour chaque exemple dans le batch.\n",
        "    \"\"\"\n",
        "    logits = raw_logits - 0.5  # Centrage des logits autour de 0 avec une marge de 0.4.\n",
        "\n",
        "    # Coût pour les valeurs positives\n",
        "    positive_cost = labels * tf.cast(tf.less(logits, margin), tf.float32) * tf.square(logits - margin)\n",
        "\n",
        "    # Coût pour les valeurs négatives\n",
        "    negative_cost = (1 - labels) * tf.cast(tf.greater(logits, -margin), tf.float32) * tf.square(logits + margin)\n",
        "\n",
        "    # Combinaison des deux composantes de la perte\n",
        "    loss = 0.5 * positive_cost + downweight * 0.5 * negative_cost\n",
        "\n",
        "    # Moyenne des pertes sur le batch\n",
        "    return tf.reduce_mean(tf.reduce_sum(loss, axis=1))"
      ],
      "metadata": {
        "id": "N3Gg3qkO9FMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_loss(y_true, y_pred_caps, y_pred_decoder):\n",
        "    # Perte pour les capsules (Margin Loss)\n",
        "    caps_loss = margin_loss(y_true, y_pred_caps)\n",
        "\n",
        "    # Perte de reconstruction (MSE)\n",
        "    recon_loss = tf.keras.losses.mean_squared_error(y_true, y_pred_decoder)\n",
        "    recon_loss = tf.reduce_mean(recon_loss)\n",
        "\n",
        "    # Ponderation des deux pertes\n",
        "    alpha = 0.0005  # Coefficient pour ajuster l'importance de la reconstruction\n",
        "    total_loss = caps_loss + alpha * recon_loss\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "GmxYGHUdi7Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data Preprocessing\n",
        "def load_dataset(data_dir, img_size, batch_size):\n",
        "    # Création des générateurs de données avec ImageDataGenerator\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1.0 / 255,  # Normalisation des pixels\n",
        "        validation_split=0.2,  # Séparation de la validation\n",
        "    )\n",
        "\n",
        "    # Création du générateur de données pour l'entraînement\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=\"categorical\",  # Sortie catégorique\n",
        "        subset=\"training\",  # Subset d'entraînement\n",
        "    )\n",
        "\n",
        "    # Création du générateur de données pour la validation\n",
        "    val_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=\"categorical\",  # Sortie catégorique\n",
        "        subset=\"validation\",  # Subset de validation\n",
        "    )\n",
        "\n",
        "\n",
        "    # Retourner les données sous forme de tuple\n",
        "    return train_generator, val_generator"
      ],
      "metadata": {
        "id": "WGv8HhcI8F22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip_file(zip_path, dest_path):\n",
        "    \"\"\"\n",
        "    Décompresse un fichier ZIP dans le répertoire de destination spécifié.\n",
        "\n",
        "    :param zip_path: Le chemin du fichier ZIP à décompresser\n",
        "    :param dest_path: Le chemin du répertoire de destination pour l'extraction\n",
        "    \"\"\"\n",
        "    # Vérifier si le fichier ZIP existe\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"Le fichier {zip_path} n'existe pas.\")\n",
        "        return\n",
        "\n",
        "    # Créer le répertoire de destination s'il n'existe pas\n",
        "    if not os.path.exists(dest_path):\n",
        "        os.makedirs(dest_path)\n",
        "\n",
        "    # Décompresser le fichier ZIP\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dest_path)\n",
        "\n",
        "    print(f\"Le fichier ZIP a été décompressé dans {dest_path}\")"
      ],
      "metadata": {
        "id": "cAsP1B3jAigP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CapsuleLayer(layers.Layer):\n",
        "    def __init__(self, num_capsules, dim_vector, num_routing=3, **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsules = num_capsules\n",
        "        self.dim_vector = dim_vector\n",
        "        self.num_routing = num_routing\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_num_capsules = input_shape[1]  # Nombre de capsules d'entrée\n",
        "        self.input_dim_vector = input_shape[2]  # Dimension de chaque vecteur de capsule d'entrée\n",
        "\n",
        "        # Matrice de poids W pour transformer les capsules d'entrée\n",
        "        self.W = self.add_weight(\n",
        "            shape=[self.input_num_capsules, self.input_dim_vector,self.num_capsules*self.dim_vector],\n",
        "            #[input_dim, input_atoms, output_dim * output_atoms]\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name='W'\n",
        "            #constraint=tf.keras.constraints.MinMaxNorm(min_value=1.0, max_value=2.0)\n",
        "        )\n",
        "        # Initialisation des biais\n",
        "        self.bias = self.add_weight(\n",
        "            shape=(self.num_capsules,self.dim_vector),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            name='bias'\n",
        "        )\n",
        "        #biases = variables.bias_variable([output_dim, output_atoms])\n",
        "    def call(self, inputs):\n",
        "\n",
        "        # Étape 1 : Calcul des votes\n",
        "        # Étendre les dimensions d'entrée et multiplier par les poids\n",
        "        input_tiled = tf.expand_dims(inputs, -1)  # Ajoute une dimension à la fin\n",
        "        input_tiled = tf.tile(input_tiled, [1, 1, 1, self.num_capsules*self.dim_vector])  # Répéter pour chaque capsule de sortie\n",
        "        votes = tf.reduce_sum(input_tiled * self.W , axis=2)  # Multiplier et réduire sur input_atoms\n",
        "        votes_reshaped = tf.reshape(votes, [-1, self.input_num_capsules, self.num_capsules, self.dim_vector])\n",
        "        input_shape = tf.shape(inputs)\n",
        "        logit_shape = tf.stack([input_shape[0], self.input_num_capsules, self.num_capsules])\n",
        "        # 🚀 Implémentation directe du routage dynamique\n",
        "        batch_size, input_dim, output_dim, output_atoms = tf.shape(votes_reshaped)[0], tf.shape(votes_reshaped)[1], tf.shape(votes_reshaped)[2], tf.shape(votes_reshaped)[3]\n",
        "        b_ij = tf.zeros([batch_size, input_dim, output_dim])\n",
        "        # Itérations pour mise à jour des connexions\n",
        "        for i in range(self.num_routing):\n",
        "          c_ij = tf.nn.softmax(b_ij, axis=2)\n",
        "          s_j = tf.reduce_sum(c_ij[..., tf.newaxis] * votes_reshaped, axis=1) + self.bias\n",
        "          v_j = squash(s_j)\n",
        "          if i < self.num_routing - 1:\n",
        "            delta_b_ij = tf.reduce_sum(votes_reshaped * v_j[:, tf.newaxis, :, :], axis=-1)\n",
        "            b_ij += delta_b_ij\n",
        "          return v_j\n",
        "\n",
        "    @staticmethod\n",
        "    def squash(vectors, axis=-1):\n",
        "        \"\"\"\n",
        "        Fonction squash pour normaliser les vecteurs des capsules.\n",
        "        \"\"\"\n",
        "        s_squared_norm = tf.reduce_sum(tf.square(vectors), axis=axis, keepdims=True)  # Norme au carré\n",
        "        scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + 1e-7)  # Normalisation\n",
        "        return scale * vectors"
      ],
      "metadata": {
        "id": "aux2t9F-94pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Définition de l'architecture du réseau\n",
        "def CapsNet(input_shape, n_class, num_routing):\n",
        "    x = layers.Input(shape=input_shape)\n",
        "\n",
        "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "    primarycaps = layers.Conv2D(32 * 8, kernel_size=9, strides=2, padding='valid', activation='relu')(conv1)\n",
        "    # Calculer la forme correcte de reshape\n",
        "    #primarycaps_shape = primarycaps.shape\n",
        "    #num_capsules = primarycaps_shape[1] * primarycaps_shape[2]  # Calcul du nombre de capsules\n",
        "    # Reshaper à (num_capsules, dim_vector) qui devrait être (32*6, 8), soit (192, 8)\n",
        "    primarycaps = layers.Reshape(target_shape=[24*24*32, 8])(primarycaps)\n",
        "    #primarycaps = layers.Lambda(lambda z: tf.sqrt(tf.reduce_sum(tf.square(z), -1)))(primarycaps)\n",
        "    #print(primarycaps.shape)\n",
        "    # Convolutional Layers\n",
        "    \"\"\"\n",
        "    primarycaps = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"valid\")(x)\n",
        "    primarycaps = layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"valid\")(primarycaps)\n",
        "    primarycaps = layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"valid\")(primarycaps)\n",
        "    # Reshape to Flattened Capsules\n",
        "    primarycaps = layers.Reshape((-1, 256))(primarycaps)\n",
        "    \"\"\"\n",
        "    # Appliquer squash avec Lambda et spécifier output_shape\n",
        "    primarycaps = layers.Lambda(squash, name='primarycap_squash')(primarycaps)\n",
        "    print('squach',primarycaps.shape)\n",
        "    # Capsule Layer\n",
        "    digitcaps = CapsuleLayer(num_capsules=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n",
        "    # Compute the norm of capsule outputs\n",
        "    outputs = layers.Lambda(lambda z: tf.norm(z, axis=-1))(digitcaps)\n",
        "    # Apply softmax activation for classification\n",
        "    #outputs = layers.Activation('softmax')(outputs)\n",
        "    model = models.Model(x, outputs)\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "EvuKZOPq9VP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement du modèle\n",
        "def train(model, data, args):\n",
        "    \"\"\"\n",
        "    Entraîne le modèle pour la première fois sans utiliser les callbacks supplémentaires.\n",
        "    \"\"\"\n",
        "    (x_train, y_train), (x_test, y_test) = data\n",
        "\n",
        "    # Compiler le modèle\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=args.lr),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'],) # Provide metrics for both outputs\n",
        "\n",
        "    # Entraîner le modèle sans callbacks\n",
        "    # Change the model.fit call to:\n",
        "    model.fit(x_train, y_train,\n",
        "          batch_size=args.batch_size, epochs=args.epochs,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "    # Sauvegarder les poids du modèle après l'entraînement\n",
        "    model.save_weights(os.path.join(args.save_dir, 'trained_model.weights.h5'))\n",
        "    print('Modèle entraîné et sauvegardé.')"
      ],
      "metadata": {
        "id": "soupHjE1-LM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement du modèle sur GPU\n",
        "def train_GPU(model, data, args):\n",
        "    \"\"\"\n",
        "    Entraîne le modèle pour la première fois sans utiliser les callbacks supplémentaires.\n",
        "    \"\"\"\n",
        "    (x_train, y_train), (x_test, y_test) = data\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=args.lr),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Spécifiez l'entraînement sur GPU\n",
        "    with tf.device('/GPU:0'):\n",
        "        model.fit(\n",
        "            x_train,\n",
        "            y_train,\n",
        "            batch_size=args.batch_size,\n",
        "            epochs=args.epochs,\n",
        "            validation_data=(x_test, y_test)\n",
        "        )\n",
        "\n",
        "    # Sauvegarder les poids du modèle après l'entraînement\n",
        "    model.save_weights(os.path.join(args.save_dir, 'trained_model.weights.h5'))\n",
        "    print('Modèle entraîné et sauvegardé.')\n"
      ],
      "metadata": {
        "id": "IW7Y-dv_Ifqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MN8EIT-_o6t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple d'utilisation\n",
        "zip_path = '/content/drive/MyDrive/EuroSAT_RGB.zip'  # Remplacez par votre chemin ZIP\n",
        "dest_path = '/content/EuroSAT_RGB'  # Remplacez par votre chemin de destination\n",
        "\n",
        "unzip_file(zip_path, dest_path)"
      ],
      "metadata": {
        "id": "iiSEbZvOAtXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths et paramètres\n",
        "#data_dir = '/content/drive/MyDrive/EuroSAT_RGB'\n",
        "data_dir = '/content/EuroSAT_RGB/EuroSAT_RGB'\n",
        "img_size = (64, 64)  # Taille de l'image\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "ZDfScwl7_jnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exécution principale\n",
        "class Args:\n",
        "  batch_size = 32\n",
        "  epochs = 100\n",
        "  lam_recon = 0.392\n",
        "  num_routing = 3\n",
        "  shift_fraction = 0.1\n",
        "  debug = 0\n",
        "  save_dir = '/content/results'\n",
        "  weights = None\n",
        "  lr = 0.01\n",
        "args = Args()"
      ],
      "metadata": {
        "id": "pmZHKDgyDMRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, val_generator = load_dataset(data_dir, img_size, batch_size)"
      ],
      "metadata": {
        "id": "pRFXvWuc-WZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Vérifie la disponibilité des périphériques GPU\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "Cbm7xv-0LmUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CapsNet(input_shape=[64, 64, 3], n_class=10, num_routing=args.num_routing)\n",
        "model.summary()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss=margin_loss,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "gEg4hkS6K31m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "history = model.fit(train_generator,\n",
        "          batch_size=args.batch_size, epochs=10,\n",
        "          validation_data=val_generator,callbacks=[callback])"
      ],
      "metadata": {
        "id": "v1tyZ0hPsOzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affiche l'historique des pertes et des précisions\n",
        "print(\"Loss History:\", history.history['loss'])\n",
        "print(\"Validation Loss History:\", history.history['val_loss'])\n",
        "print(\"Accuracy History:\", history.history['accuracy'])\n",
        "print(\"Validation Accuracy History:\", history.history['val_accuracy'])"
      ],
      "metadata": {
        "id": "ZD4-Pyge2dnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Affichez la courbe de perte\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Affichez la courbe de précision\n",
        "if 'accuracy' in history.history:\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy over epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "fEctPlYHlvtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenez les prédictions du modèle\n",
        "predictions = model.predict(val_generator)\n",
        "\n",
        "# Pour un problème de classification, convertissez les prédictions en classes\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Obtenez les vraies étiquettes\n",
        "true_classes = val_generator.classes\n",
        "class_labels = list(val_generator.class_indices.keys())\n",
        "\n",
        "# Afficher quelques prédictions avec les images\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    img, label = val_generator[i]\n",
        "    plt.imshow(img[0])\n",
        "    true_label = class_labels[true_classes[i]]\n",
        "    predicted_label = class_labels[predicted_classes[i]]\n",
        "    plt.title(f\"True: {true_label}\\nPred: {predicted_label}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "s0Cc4ytdleNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarder les poids du modèle après l'entraînement\n",
        "model.save_weights(os.path.join('/content/drive/MyDrive/weights', 'trained_model.weights.h5'))\n",
        "print('Modèle entraîné et sauvegardé.')"
      ],
      "metadata": {
        "id": "_4ldjrHuQHkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_C5iRSILQIiO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}